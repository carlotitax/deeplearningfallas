{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workshop_02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofZ3aKltlq0e",
        "colab_type": "text"
      },
      "source": [
        "# MFPT Classification Model\n",
        "\n",
        "Continuando con el procesamiento de datos de la clase anterior, en este workshop volveremos a trabajar con el [Bearing Fault Dataset](https://www.mfpt.org/fault-data-sets/) de la MFPT. No obstante, ahora nos concentraremos en la construcción e implementación de modelos de clasificación de deep learning mediante `TensorFlow`.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/deeplearningfallas/master/workshop_02/bin/tensorflow.png\" width=\"400\">\n",
        "\n",
        "`TensorFlow`, en términos generales, consiste en un framework diseñado para desarrollar e implementar algoritmos de Machine Learnine, y por supuesto, entre ellos, modelos de Deep Learning. Una de las particulares de este framework es que ofrece toda una gama de niveles de abstracción, desde el desarrollo de modelos de mayor complejidad mediante herramientas `low-level`, hasta la compilación y el entrenamiento de arquitecturas mediante estructuras `high-level`, como la API Keras.\n",
        "\n",
        "Puede encontrar la documentación de estas librerías en los siguientes links:\n",
        "- https://www.tensorflow.org/api_docs/python/\n",
        "- https://keras.io/api/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JTPegUAUYdN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "ca5a27da-55c3-483a-cdbd-2486f7b4c3e0"
      },
      "source": [
        "!git clone https://github.com/cherrerab/deeplearningfallas.git\n",
        "%cd /content/deeplearningfallas"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deeplearningfallas'...\n",
            "remote: Enumerating objects: 68, done.\u001b[K\n",
            "remote: Counting objects: 100% (68/68), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 202 (delta 25), reused 0 (delta 0), pack-reused 134\u001b[K\n",
            "Receiving objects: 100% (202/202), 2.47 MiB | 18.31 MiB/s, done.\n",
            "Resolving deltas: 100% (70/70), done.\n",
            "/content/deeplearningfallas\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNFS3Gf3K503",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Training Data\n",
        "\n",
        "Nuevamente debemos preparar y reestructurar los datos del dataset para generar los sets de entrenamiento `(X_train, Y_train)` y validación `(X_val, Y_val)` que utilizaremos para entrenar nuestros modelos. Del mismo modo que en el workshop anterior, extraremos ventanas de datos de las series del dataset mediante la función `get_time_windows`. No obstante, modificaremos esta para permitir que exista superposición o un traslapo entre las ventanas. De esta manera será posible extraer más ventanas desde las series originales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jbz8qU53llbC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "b2fd6458-6d62-47df-82c8-8cbc91f47768"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def get_time_windows(data, nperwd, nleap):\n",
        "  \"\"\"\n",
        "  -> np.array\n",
        "\n",
        "  generates a numpy array of time windows, of length nperwd, extracted\n",
        "  from data.\n",
        "\n",
        "  :param pd.Series data:\n",
        "    time series of measurement values.\n",
        "  :param int nperwd:\n",
        "    length of samples of each time window.\n",
        "  :param int nleap:\n",
        "    length of leap between time windows.\n",
        "\n",
        "  :returns:\n",
        "    a numpy array of size (n_windows, nperwd).\n",
        "  \"\"\"\n",
        "\n",
        "  # obtener np.array de la serie de datos\n",
        "  x = data.values\n",
        "  n_data = x.shape[0]\n",
        "  \n",
        "  # determinar cantidad de ventanas a generar\n",
        "  n_windows = np.floor( (n_data - nperwd)/nleap ) + 1\n",
        "  n_windows = int(n_windows)\n",
        "\n",
        "  # inicializar dataset\n",
        "  X = np.zeros( (n_windows, nperwd) )\n",
        "  \n",
        "  # generar time windows\n",
        "  for i in range(n_windows):\n",
        "    # obtener index de la ventana\n",
        "    idx_start, idx_end = i*nleap, i*nleap + nperwd\n",
        "\n",
        "    # asignar datos a X\n",
        "    X[i, :] = x[idx_start:idx_end]\n",
        "  \n",
        "  return X\n",
        "\n",
        "# print ejemplo con serie de 0 a 15\n",
        "x = pd.Series( np.arange(0, 16) )\n",
        "print('data series:\\n', x.values)\n",
        "\n",
        "X = get_time_windows(x, 5, 2)\n",
        "print('\\ntime windows:\\n', X)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data series:\n",
            " [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
            "\n",
            "time windows:\n",
            " [[ 0.  1.  2.  3.  4.]\n",
            " [ 2.  3.  4.  5.  6.]\n",
            " [ 4.  5.  6.  7.  8.]\n",
            " [ 6.  7.  8.  9. 10.]\n",
            " [ 8.  9. 10. 11. 12.]\n",
            " [10. 11. 12. 13. 14.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ3JLlPHO86E",
        "colab_type": "text"
      },
      "source": [
        "Ahora, antes de procesar los datos con esta nueva función, debemos tener cuidado con que no se produzca `data leakage` entre nuestros sets. El `Data Leakage` ocurre cuando datos del `val set` o el `test set` se repiten o se filtran en el `train set`. De esta forma, incluso cuando el modelo cae en overfitting durante el entrenamiento, su desempeño sobre los datos de testing no se ve perjudicado.\n",
        "\n",
        "Así, para evitar esto y confiando en la uniformidad de los datos en el tiempo, dividiremos cada una de las series en dos bloques a partir de los cuales extraeremos las ventanas de tiempo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7cxUL53EqZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ubicación del dataset\n",
        "dst_path = 'workshop_01//MFPT_raw_dst.csv'\n",
        "\n",
        "# importar a un pd.DataFrame\n",
        "db = pd.read_csv(dst_path)\n",
        "\n",
        "# obtener cantidad de datos que corresponden al 80%\n",
        "idx = \n",
        "\n",
        "# obtener bloques de entrenamiento y validación\n",
        "x = db.values\n",
        "x_train, x_val = \n",
        "\n",
        "# reestructurar DataFrames\n",
        "colnames = ['Time', 'Healthy', 'Outer Race', 'Inner Race']\n",
        "db_train = pd.DataFrame( x_train, columns=colnames )\n",
        "db_val = pd.DataFrame( x_val, columns=colnames )\n",
        "\n",
        "print( 'train split: {:d} data points'.format(db_train.shape[0]) )\n",
        "print( 'validation split: {:d} data points'.format(db_val.shape[0]) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfOhPlYMXbTt",
        "colab_type": "text"
      },
      "source": [
        "Hecho esto, ahora podemos extraer las ventanas de datos sin preocuparnos del `data leakage` que podría ocurrir con la superposición entre las ventanas. Análogamente al workshop anterior utilizaremos un `nperwd` de `96` y un `nleap` de `48` (50% de superposición)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxy6mIPmXDss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# definir tamaño de ventanas\n",
        "nperwd = \n",
        "nleap =\n",
        "\n",
        "# generar ventanas a partir de los datos Healthy\n",
        "train_HB =\n",
        "val_HB =\n",
        "\n",
        "# generar ventanas a partir de los datos Outer Race\n",
        "train_OR =\n",
        "val_OR =\n",
        "\n",
        "# generar ventanas a partir de los datos Inner Race\n",
        "train_IR =\n",
        "val_IR =\n",
        "\n",
        "# concatenar estas ventanas mediante np.vstack\n",
        "X_train = \n",
        "X_val = "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVhkXP2xa1eX",
        "colab_type": "text"
      },
      "source": [
        "Por supuesto, como nuestro modelo debe aprender a clasificar el estado de salud de nuetros samples, debemos crear un np.array target que contenga las etiquetas de nuestras muestras. A diferencia del workshop anterior, en este caso utilizaremos `keras.utils.to_categorical` para transformar nuestras etiquetas a `one hot encoding`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k826oXIKb3Fm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "# generar np.array Y_train con las etiquetas correspondientes\n",
        "Y_train = [0]*train_HB.shape[0] + [1]*train_OR.shape[0] + [2]*train_IR.shape[0]\n",
        "Y_train = np.reshape( np.array(Y_train), (-1, 1) )\n",
        "Y_train = \n",
        "\n",
        "# generar np.array Y_val con las etiquetas correspondientes\n",
        "Y_val = [0]*val_HB.shape[0] + [1]*val_OR.shape[0] + [2]*val_IR.shape[0]\n",
        "Y_val = np.reshape( np.array(Y_val), (-1, 1) )\n",
        "Y_val = "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGLVIErm2cIX",
        "colab_type": "text"
      },
      "source": [
        "Los archivos `.npz` permiten guardar múltiples sets o `np.arrays` en un mismo archivo, lo cual resulta bastante útil a la hora de registrar resultados o compartir/enviar datasets. Para manipular este tipo de archivos, Numpy cuenta con los métodos `np.savez` y `np.load`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eK3TQQXd6u6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# guardar archivo MFPT_raw.npz con los sets de datos\n",
        "np.savez('//content//MFPT_raw.npz',\n",
        "         X_train=X_train, Y_train=Y_train,\n",
        "         X_val=X_val, Y_val=Y_val)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSlM9sKX3p6W",
        "colab_type": "text"
      },
      "source": [
        "## Signal Features Data\n",
        "\n",
        "Para tener un set con el cual poder comparar desempeños, generaremos los sets de entrenamiento a partir de las features extraidas de la señal original. De la misma manera que en el workshop anterior, utilizaremos la función `extract_features` para llevar esto a cabo. No obstante, vale la pena notar que esta función extrae ahora 9 métricas a partir de las ventanas de datos. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEGNiH_DfpmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats import kurtosis\n",
        "from scipy.stats import skew\n",
        "\n",
        "def extract_features(x):\n",
        "  \"\"\"\n",
        "  -> np.array\n",
        "\n",
        "  compute 9 signal features for each sample along the data x:\n",
        "  - mean, variance.\n",
        "  - rms, peak, valley, peak2peak.\n",
        "  - crest factor, kurtosis, skewness.\n",
        "\n",
        "  :param np.array x:\n",
        "    data of shape (n_samples, nperwd) containing de samples.\n",
        "\n",
        "  :returns:\n",
        "    np.ndarray of shape (n_samples, n_features) containing\n",
        "    the extracted features.\n",
        "  \"\"\"\n",
        "\n",
        "  # mean\n",
        "  mean = np.mean( x, axis=1 )\n",
        "  mean = mean.reshape( (-1, 1) )\n",
        "\n",
        "  # varianza\n",
        "  var = np.var( x, axis=1 )\n",
        "  var = var.reshape( (-1, 1) )\n",
        "\n",
        "  # -----------------------------------------------------------\n",
        "  # valor eficaz rms\n",
        "  rms = np.sqrt( np.mean( np.square(x), axis=1 ) )\n",
        "  rms = rms.reshape( (-1, 1) )\n",
        "\n",
        "  # peak\n",
        "  peak = np.reshape( np.max( x, axis=1 ), (-1, 1) )\n",
        "\n",
        "  # valley\n",
        "  valley = np.reshape( np.min( x, axis=1 ), (-1, 1) )\n",
        "\n",
        "  # peak2peak\n",
        "  p2p = np.reshape( np.abs( peak - valley ), (-1, 1) )\n",
        "\n",
        "  # -----------------------------------------------------------\n",
        "  # crest factor\n",
        "  cf = np.divide( peak, rms )\n",
        "\n",
        "  # kurtosis\n",
        "  ktsis = np.reshape( kurtosis( x, axis=1 ), (-1, 1) )\n",
        "\n",
        "  # skewness\n",
        "  skwn = np.reshape( skew( x, axis=1 ), (-1, 1) )\n",
        "\n",
        "  # -----------------------------------------------------------\n",
        "  # concatenar features\n",
        "  out = np.hstack( [mean, var, rms, peak, valley, p2p, cf, ktsis, skwn] )\n",
        "  return out\n",
        "\n",
        "# obtener sets de 27 features\n",
        "X_train_fts = extract_features( X_train )\n",
        "X_val_fts = extract_features( X_val )\n",
        "\n",
        "# guardar archivo MFPT_27features.npz\n",
        "np.savez('//content//MFPT_features.npz',\n",
        "         X_train=X_train_fts, Y_train=Y_train,\n",
        "         X_val=X_val_fts, Y_val=Y_val)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdYZfkg6hAQM",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Model Building\n",
        "\n",
        "Como se mencionó en un principio, para configurar nuestros modelos utilizaremos principalmente la librería `keras` o `tf.keras`. Keras es una API de alto nivel para la creación y el entrenamiento de modelos de deep learning. Está orientada y diseñada para la construcción de modelos de forma modular o en bloques. De este modo, ofrece un framework mucho más amigable e intuitivo para principiantes, a la vez que mantiene un estructura personalizable y versátil que permite a usuarios más avanzados incorporar nuevas ideas.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/deeplearningfallas/master/workshop_02/bin/keras_logo.png\" width=\"400\">\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Slua6HCsf0V",
        "colab_type": "text"
      },
      "source": [
        "## Model Setup\n",
        "Los elemenos básicos para la construcción de un `keras.Model` consisten en las capas o `layers` del modelo. En este sentido, configurar un modelo en Keras resulta en ir uniendo o conectando capas `keras.layers`.\n",
        "\n",
        "Para comenzar e introducir el framework de esta librería, construiremos un modelo o red neuronal `Sequential` a partir de únicamente capas `keras.layers.Dense` y otras capas elementales.\n",
        "- https://keras.io/api/layers/\n",
        "- https://keras.io/api/layers/activations/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrReGsv2g-uq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "\n",
        "# inicializar modelo keras.Sequential\n",
        "model = Sequential()\n",
        "\n",
        "# ahora podemos ir agregando secuencialmente capas nuestro modelo\n",
        "# mediante el método keras.Model.add\n",
        "\n",
        "# ---\n",
        "# primero debemos agregar nuestra capa Input donde debemos especificar\n",
        "# las dimensiones de los datos que se ingresarán al modelo\n",
        "input_dim = ( , )\n",
        "model.add( Input( shape= ) )\n",
        "\n",
        "# ---\n",
        "# ahora debemos ir agregando nuestras capas Dense.\n",
        "# https://keras.io/api/layers/core_layers/dense/\n",
        "\n",
        "# las keras.layers.Dense reciben la cantidad de nodos o units dentro\n",
        "# de la capa y la función de activación que operarán.\n",
        "# https://keras.io/api/layers/activations/\n",
        "\n",
        "model.add( Dense( units=, activation= ) )\n",
        "model.add(  )\n",
        "\n",
        "# ---\n",
        "# por último debemos configurar nuestra capa de salida\n",
        "# dado que el modelo consiste en uno de clasificación emplearemos\n",
        "# la función softmax, donde cada nodo indicará la probabilidad de que\n",
        "# los datos correspondan a una de las etiquetas o estados de salud.\n",
        "labels_num = \n",
        "model.add( Dense(units=labels_num, activation=) )\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KDJNziMspZN",
        "colab_type": "text"
      },
      "source": [
        "Así, hemos construido nuestro modelo de clasificación el cual consiste de una capa de entrada `Input`, dos capas ocultas Fully Connected o `Dense` con 32 nodos de activación ReLU cada una, y una capa de salida `Dense` de 3 nodos con función de activación SoftMax. De esta manera, cada uno de los nodos de salida estará asociado a un estado de salud.\n",
        "\n",
        "Para imprimir información sobre el modelo generado, Keras cuenta con el método `keras.Model.summary` para presentar un resumen de la arquitectura de la red neuronal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLrPmpMJuFVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imprimir object o clase del modelo\n",
        "print('type(model): ', type(model))\n",
        "\n",
        "# imprimir resumen del modelo\n",
        "print('\\nMFPT classification model summary:\\n')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geUVcZb1vE_t",
        "colab_type": "text"
      },
      "source": [
        "## Compile Model\n",
        "Antes de poner a entrenar al modelo, es necesario realizar unas configuraciones adicionales. En particular, debemos especificar la función de pérdida o `loss function` que se optimizará durante el entrenamiento y el método de optimización como SGD o Adam.\n",
        "- https://keras.io/api/models/model_training_apis/\n",
        "- https://keras.io/api/optimizers/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BrQRyi4vydP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "# configurar optimizador Adam\n",
        "# https://keras.io/api/optimizers/adam/\n",
        "opt = \n",
        "\n",
        "# ---\n",
        "# compilar modelo siguiendo como función de pérdida\n",
        "# la categorical crossentropy\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics = ['accuracy'])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYJEA9Kdw33-",
        "colab_type": "text"
      },
      "source": [
        "## Model Training\n",
        "Hemos llegado a la parte final del proceso, para entrenar nuestro modelo debemos especificar los sets que utilizaremos para el proceso `(X_train, Y_train)`, la cantidad de `epochs` que durará el entrenamiento, y el `batch size` de muestras que se irán entregando al modelo a medida que este va iterativamente ajustando sus parámetros.\n",
        "\n",
        "Para entrenar `keras.Models` se utiliza el método `keras.Model.fit`, el cual aparte de iniciar y realizar la rutina de entrenamiento, retorna un registro `History`. Mediante `History.history` es posible acceder a la evolución de la función de pérdida durante el entrenamiento tanto sobre los datos de `train` como sobre los de `validation`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y11ZGqzyE8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cargar archivo .npz que contiene los sets de entrenamiento\n",
        "npz_dst = np.load('//content//MFPT_raw.npz')\n",
        "\n",
        "# extraer sets de datos\n",
        "X_train, Y_train = npz_dst['X_train'], npz_dst['Y_train']\n",
        "X_val, Y_val = npz_dst['X_val'], npz_dst['Y_val']\n",
        "\n",
        "model_history = model.fit(X_train, Y_train,\n",
        "                          batch_size=, epochs=,\n",
        "                          validation_data=)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQBAnJWX1aAr",
        "colab_type": "text"
      },
      "source": [
        "## Model Evaluation\n",
        "Finalmente, una vez entrenado nuestro modelo debemos evaluar su desempeño. En este caso particular, dada la poca cantidad de datos, utilizaremos los datos de validación como datos de testing. Para utilizar el `keras.Model` sobre nuevos datos de clasificación, conviene utilizar el método `keras.Sequential.predict`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Cd8O0fn2lSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# obtener predicciones de X_val\n",
        "Y_pred = \n",
        "\n",
        "# para que el resultado nos sea más intuitivo transformaremos\n",
        "# las etiquetas nuevamente a non one-hot-encoding\n",
        "# utilizando np.argmax\n",
        "Y_pred = \n",
        "Y_true = \n",
        "\n",
        "# calcular accuracy de la clasificación.\n",
        "accuracy = accuracy_score(Y_true, Y_pred)\n",
        "print('validation accuracy: {:1.3f}'.format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIeyGS4g4X9o",
        "colab_type": "text"
      },
      "source": [
        "Por otro lado, podemos hacer uso de herramientas gráficas para evaluar el desempeño de los modelos. En este caso, utilizaremos el gráfico de función de pérdida y la matriz de confusión.\n",
        "\n",
        "- Gráfico de función de pérdida: La función de pérdida o `loss function` es el paramétro que se va optimizando a medida que la red se entrena. El modelo va ajustando los pesos de ponderación entre los nodos tal de minimizar este paramétro. En el gráfico de función de perdida se puede visualizar el desempeño del entrenamiento de el modelo y además la convergencia entre la curva de training y validation. Si la curva de validation se escapa considerablemente de la curva de training esto es un indicador de que el modelo ha sufrido overfitting y por tanto ha perdido generalidad (se aprende los datos de training de memoria).\n",
        "\n",
        "- Matriz de confusión: Esta tabla permite comparar las predicciones del modelo versus las etiquetas reales de los datos.\n",
        "\n",
        "Ambos métodos, `plot_loss_function` y `plot_confusion_matrix`, se encuentran disponibles en el módulo `utils` del github del curso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jS6DTrR-LkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from utils import plot_loss_function, plot_confusion_matrix\n",
        "\n",
        "# plot gráfico de función de pérdida\n",
        "plot_loss_function(model_history, figsize=(10,4))\n",
        "\n",
        "# plot de matriz de confusión\n",
        "plot_confusion_matrix(Y_true, Y_pred, ['Healthy', 'Outer Race', 'Inner Race'], figsize=(7, 7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V14hUfO0BUlR",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Model Build Template\n",
        "Para poder ir modificando los hiperparámetros de nuestro modelo de manera más cómoda, a continuación se entrega un template que compila los bloques de código anteriores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz8HgPhGBofr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from utils import plot_loss_function, plot_confusion_matrix\n",
        "\n",
        "# cargar archivo .npz que contiene los sets de entrenamiento\n",
        "npz_dst = np.load('//content//MFPT_raw.npz')\n",
        "\n",
        "# extraer sets de datos\n",
        "X_train, Y_train = npz_dst['X_train'], npz_dst['Y_train']\n",
        "X_val, Y_val = npz_dst['X_val'], npz_dst['Y_val']\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# inicializar modelo keras.Sequential\n",
        "model = Sequential()\n",
        "\n",
        "# agregar capa Input\n",
        "input_dim = (X_train.shape[1], )\n",
        "model.add( Input( shape=input_dim ) )\n",
        "\n",
        "# ahora debemos ir agregando nuestras capas Dense.\n",
        "model.add( Dense(units=32, activation='relu') )\n",
        "model.add( Dense(units=32, activation='relu') )\n",
        "\n",
        "# por último debemos configurar nuestra capa de salida\n",
        "labels_num = 3\n",
        "model.add( Dense(units=labels_num, activation='softmax') )\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# compilar modelo\n",
        "opt = Adam(learning_rate=1e-3)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics = ['accuracy'])\n",
        "\n",
        "# entrenar modelo\n",
        "model_history = model.fit(X_train, Y_train, batch_size=32, epochs=400,\n",
        "                          validation_data = (X_val, Y_val), verbose=0)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# obtener predicciones de X_val\n",
        "Y_pred = model.predict(X_val)\n",
        "\n",
        "# transformar a non one-hot-encoding.\n",
        "Y_pred = np.argmax( Y_pred, axis=-1 )\n",
        "Y_true = np.argmax( Y_val, axis=-1 )\n",
        "\n",
        "# calcular accuracy de la clasificación.\n",
        "accuracy = accuracy_score(Y_true, Y_pred)\n",
        "print('validation accuracy: {:1.3f}\\n'.format(accuracy))\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# plot gráfico de función de pérdida\n",
        "plot_loss_function(model_history, figsize=(10,4))\n",
        "\n",
        "# plot de matriz de confusión\n",
        "plot_confusion_matrix(Y_true, Y_pred, ['Healthy', 'Outer Race', 'Inner Race'], figsize=(7, 7))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}