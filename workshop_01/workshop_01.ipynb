{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workshop_01",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6t2FXvxTJlD",
        "colab_type": "text"
      },
      "source": [
        "# MFPT Fault Dataset Manipulation\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/deeplearningfallas/master/workshop_01/bin/mfpt_logo.png\" width=\"400\">\n",
        "\n",
        "La Machinery Failure Prevention Technology es una organización orientada a la integración y al intercambio de información técnica entre distintas comunidades científicas y de ingeniería, con el fin de avanzar en el estudio de los mecánismos de falla.\n",
        "\n",
        "https://www.mfpt.org/\n",
        "\n",
        "En este workshop trabajaremos con su [Bearing Fault Dataset](https://www.mfpt.org/fault-data-sets/), el cual contiene mediciones experimentales de vibración de rodamientos de bola bajo distintos estados de daño. Fallas localizadas en este tipo de componentes pueden ocurrir en la ranura del anillo exterior, en la ranura del anillo interior, o bien, en alguno de los elementos rodantes del rodamiento. Cuando los elementos rodantes pasan o impactan con alguno de estos defectos, se producen vibraciones de alta frecuencia que pueden ser registradas, por ejemplo, mediante un acelerómetro o un transductor.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/deeplearningfallas/master/workshop_01/bin/bearing_example.png\" width=\"500\">\n",
        "\n",
        "El propósito de este dataset es propiciar la investigación y el desarrollo de modelos capaces de detectar e identificar el tipo de falla en un rodamiento, en base a las señales medidas y registradas por un transductor de vibración.\n",
        "\n",
        "El dataset que se usará en este workshop consiste en tres señales de aceleración (g) registradas durante tres segundos (2.99 seg) a un sample rate de 4882.8 Hz sobre rodamientos NICE® a 1500 rpm. Estos registros se corresponden con tres estados de salud: `healthy`, `outer race fault`, `inner race fault`.\n",
        "\n",
        "El dataset se encuentra en el archivo `MFPT_raw_dst.csv` contenido en el github del curso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pW_lnygRdUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/cherrerab/deeplearningfallas.git\n",
        "%cd /content/deeplearningfallas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INaze5b6pBDP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "# Exploración Inicial\n",
        "\n",
        "Comenzemos por importar el dataset a un `pandas.DataFrame` para observar su contenido. El archivo se encuentra en el path `\\content\\deeplearningfallas\\workshop_01\\MFPT_raw_dst.csv`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yC2NWTiF2lM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ubicación del dataset\n",
        "dst_path = 'workshop_01//MFPT_raw_dst.csv'\n",
        "\n",
        "# importar a un pd.DataFrame\n",
        "\n",
        "\n",
        "# imprimir head del dataset (primeras 5 filas)\n",
        "\n",
        "\n",
        "# imprimir cantidad de datos contenidos\n",
        "n_data = \n",
        "print( '\\nsamples: ', n_data )\n",
        "\n",
        "# imprimir tiempo total registrado\n",
        "t_total = \n",
        "print( '\\ntime total: {:1.7f} seg'.format(t_total) )\n",
        "\n",
        "# imprimir time step y sample rate de la señal\n",
        "t_step = \n",
        "print( '\\ntime step: {:1.7f} seg'.format(t_step) )\n",
        "\n",
        "sample_rate = \n",
        "print( '\\nsample rate: {:1.1f} Hz'.format(sample_rate) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzJfcaXHOqWh",
        "colab_type": "text"
      },
      "source": [
        "Como se puede ver, el dataset cuenta con 14,649 samples de aceleración registrados durante 3 segundos de medición. Por supuesto, el dataset cuenta con datos de tres estados distintos de salud.\n",
        "\n",
        "Ahora, parte importante de una buena exploración de datos consiste en visualizarlos, esto nos permitirá manejar una mejor intuición del comportamiento y los patrones que estos presenten, así como sus posibles anomalías. Para esto, usaremos principalmente `matplotlib`, la cual es una librería orientada y especializada en la creación de gráficos y visualizaciones en python.\n",
        "\n",
        "https://matplotlib.org/\n",
        "\n",
        "De este modo, lo primero que haremos será plotear los datos de aceleración de cada uno de los estados de aceleración en el tiempo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0b7qE1lQ-O3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# gráfico de estado de salud Healthy\n",
        "plt.figure( figsize=(15, 4) )\n",
        "plt.title('Healthy')\n",
        "plt.xlabel('Time (seg)'); plt.ylabel('Acceleration (g)')\n",
        "\n",
        "# gráfico de estado de salud Outer Race\n",
        "\n",
        "\n",
        "# gráfico de estado de salud Inner Race\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAqCyTFJV8Jk",
        "colab_type": "text"
      },
      "source": [
        "Se puede observar que existe un diferencia evidente entre las tres señales, donde la magnitud de la aceleración es lo que cambia de forma más notoria. Podemos cuantificar esta diferencia fácilmente mediante `np.max` y `np.abs`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am8NSbmId9O1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# obtendremos la magnitud de la señal en cada sample mediante np.abs\n",
        "db_abs = \n",
        "\n",
        "# de este modo, podremos obtener la amplitud máxima de cada señal\n",
        "# mediante np.max o pd.DataFrame.max\n",
        "db_max = \n",
        "print( db_max )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA3rYzGH7cSN",
        "colab_type": "text"
      },
      "source": [
        "## Spectrograms\n",
        "\n",
        "Ahora, dado que los datos consisten en señales de vibración, puede ser útil para la exploración analizar los espectrogramas de cada una. Por supuesto, en python practicamente ya está todo implementado y solo basta buscar la librería adecuada. Una rápida búsqueda por google nos lleva a `scipy.signal.spectrogram`.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/deeplearningfallas/master/workshop_01/bin/scipy_badge.png\" width=\"350\">\n",
        "\n",
        "Dado que debemos crear tres espectrogramas, uno para cada serie temporal, crearemos la función `MFPT_spectrogram` que nos permitirá repetir el proceso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnPYnPeZRSfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import signal\n",
        "\n",
        "def MFPT_spectrogram(data, sr=1.0, nperwd=256):\n",
        "  \"\"\"\n",
        "  -> None\n",
        "\n",
        "  compute and plot the spectrogram of the time series.\n",
        "  spectrograms can be used as a way of visualizing the change of a\n",
        "  nonstationary signal’s frequency content over time.\n",
        "\n",
        "  :param pd.Series data:\n",
        "    time series of measurement values.\n",
        "  :param float sr:\n",
        "    sample rate of the time series data. defaults to 1.0.\n",
        "  :param int nperwd:\n",
        "    length of samples of each time window. defaults to 256.\n",
        "\n",
        "  :returns:\n",
        "    None\n",
        "  \"\"\"\n",
        "\n",
        "  # obtener espectrograma\n",
        "  freqs, times, spectrogram = signal.spectrogram(data, fs=sr, nperseg=nperwd)\n",
        "\n",
        "  # crear meshgrid para plot\n",
        "  X, Y = np.meshgrid(times, freqs)\n",
        "\n",
        "  # configurar plot\n",
        "  plt.figure( figsize=(15, 4) )\n",
        "  plt.title( data.name )\n",
        "  plt.xlabel('Time (seg)'); plt.ylabel('Frecueny (Hz)')\n",
        "\n",
        "  # plot\n",
        "  plt.contourf( X, Y, spectrogram, levels=256, cmap='magma')\n",
        "  plt.colorbar()\n",
        "\n",
        "  return None\n",
        "\n",
        "# obtener espectrograma Healthy\n",
        "\n",
        "\n",
        "# obtener espectrograma Outer Race\n",
        "\n",
        "\n",
        "# obtener espectrograma Inner Race\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZXCxVt5D8nO",
        "colab_type": "text"
      },
      "source": [
        "Si bien en el caso `healthy` el espectrograma indica que la señal consiste de básicamente ruido/noise, tanto en el caso `outer race fault` como en el `inner race fault` se distinguen patrones que indican la presencia de anomalías en la señal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gTuVoV5FOQi",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Training Data\n",
        "Eventualmente, lo que queremos hacer con este dataset es generar un modelo de deep learning que, a partir de una muestra o sample de una señal de vibración, sea capaz de identificar el estado de salud o tipo de falla que presente el componente. Es decir un modelo de clasificación.\n",
        "\n",
        "Ahora bien, para generar este modelo de clasificación, es necesario contar con una serie de muestras para su entrenamiento. Durante este entrenamiento, el modelo observará estas muestras e iterativamente irá ajustando sus pesos de tal manera de reducir su error de clasificación. Es de esta manera que el modelo logra aprender de los datos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7aS1w7fcBCg",
        "colab_type": "text"
      },
      "source": [
        "## Time Windows\n",
        "\n",
        "En general, mientras más muestras se posea para el entrenamiento, mejor. No obstante, nunca contaremos con datasets infinitos por lo que debemos ser precavidos y eficientes respecto a como distribuimos nuestros datos. En este caso particular, contamos con tres señales finitas a partir de las cuales tendremos que extraer nuestras muestras. De este modo, nuestras muestras consistirán en ventanas de datos o tiempo, similar a como los espectrogramas anteriores procesan la información.\n",
        "\n",
        "A continuación, definiremos la función `get_time_windows` que nos permita extraer las ventanas de tiempo de una serie de datos, en función de un tamaño de ventana `nperwd` definido."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOMce8B3QsF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_time_windows(data, nperwd):\n",
        "  \"\"\"\n",
        "  -> np.array\n",
        "\n",
        "  generates a numpy array of time windows, of length nperwd, extracted\n",
        "  from data.\n",
        "\n",
        "  :param pd.Series data:\n",
        "    time series of measurement values.\n",
        "  :param int nperwd:\n",
        "    length of samples of each time window.\n",
        "  :param int overlap:\n",
        "    length of overlap between time windows.\n",
        "\n",
        "  :returns:\n",
        "    a numpy array of size (n_windows, nperwd).\n",
        "  \"\"\"\n",
        "\n",
        "  # obtener np.array de la serie de datos\n",
        "  \n",
        "\n",
        "  # determinar cantidad de ventanas a generar\n",
        "  \n",
        "\n",
        "  # generar time windows\n",
        "  \n",
        "\n",
        "  return out\n",
        "\n",
        "# generar ventanas a partir de los datos Inner Race\n",
        "X_IR = get_time_windows( db['Inner Race'], nperwd=128 )\n",
        "\n",
        "print('time windows generated: {:d}'.format(X_IR.shape[0]))\n",
        "\n",
        "# plot muestra de ventana\n",
        "x = \n",
        "\n",
        "plt.figure( figsize=(15, 4) )\n",
        "plt.title('Inner Race TW 12')\n",
        "plt.xlabel('Data point'); plt.ylabel('Acceleration (g)')\n",
        "plt.plot( x )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLx4xEmyaB-G",
        "colab_type": "text"
      },
      "source": [
        "Una pregunta que nos podríamos hacer ahora, es respecto a de qué tamaño deben ser estas ventanas. La intuición nos indica que mientras más ventanas generemos, mejor será nuestro entrenamiento. No obstante, si definimos `nperwd` muy pequeño es posible que nuestras ventanas no contengan suficiente información.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/deeplearningfallas/master/workshop_01/bin/nperwd_panik.png\" height=\"300\">\n",
        "\n",
        "De esta manera, debemos encontrar un `nperwd` que logre un buen balance entre la cantidad de ventanas o samples generados, y la cantidad de información que estos contengan. Por supuesto, una forma de hacer esto es mediante prueba y error, pero también es posible explorar métodos un tanto más sistemáticos.\n",
        "\n",
        "Dado que este es el primer workshop del curso, nos limitaremos a definir el `nperwd` que nos parezca adecuado, por ejemplo, `48`. Ahora crearemos el `training set` para nuestro modelo, utilizando las tres señales de vibración."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzCNHeeCiDag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# definir tamaño de ventanas\n",
        "nperwd = 48\n",
        "\n",
        "# generar ventanas a partir de los datos Healthy\n",
        "X_HB = \n",
        "\n",
        "# generar ventanas a partir de los datos Outer Race\n",
        "X_OR = \n",
        "\n",
        "# generar ventanas a partir de los datos Inner Race\n",
        "X_IR = \n",
        "\n",
        "# finalmente, concatenar estas ventanas mediante np.vstack\n",
        "X_raw = \n",
        "\n",
        "print('number of samples: ', X_raw.shape[0])\n",
        "print('samples length: ', X_raw.shape[1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnIrIwUgj4Zv",
        "colab_type": "text"
      },
      "source": [
        "Por supuesto, como nuestro modelo debe aprender a clasificar el estado de salud de nuetros `samples`, debemos crear un `np.array` `target` que contenga las etiquetas de nuestras muestras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXUor_92le3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generar np.array labels con las etiquetas correspondientes\n",
        "labels = \n",
        "\n",
        "# reshape a dimensiones compatibles con X_raw\n",
        "Y = labels.reshape( (-1, 1) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_2bJVi3ndMO",
        "colab_type": "text"
      },
      "source": [
        "## Feature Extraction\n",
        "\n",
        "Hasta este punto, salvo quizás por los espectrogramas, nos hemos limitado a utilizar únicamente los datos puros o `raw`. No obstante, en Machine Learning, es común preprocesar los datos antes de entrenar los modelos y no utilizar los datos en `raw`. El objetivo de esto es extraer `features` y valores que sean más efectivos a la hora del entrenamiento.\n",
        "\n",
        "Para introducir esta etapa de preprocesamiento, definiremos la función `extract_features`. Esta función procesará las ventanas de datos generadas en la sección anterior, y calculará algunas `features` características de señales de vibración: `rms`, `peak`, `valley`, `peak2peak`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE4urOsys2Ce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_features(x):\n",
        "  \"\"\"\n",
        "  -> pd.DataFrame\n",
        "\n",
        "  compute signal features for each sample in the data x.\n",
        "\n",
        "  :param np.array x:\n",
        "    data of shape (n_samples, nperwd) containing de samples.\n",
        "\n",
        "  :returns:\n",
        "    pd.DataFrame of shape (n_samples, n_features) containing\n",
        "    the extracted features.\n",
        "  \"\"\"\n",
        "  \n",
        "  # valor eficaz rms\n",
        "  rms = \n",
        "\n",
        "  # peak\n",
        "  peak = \n",
        "\n",
        "  # valley\n",
        "  valley = \n",
        "\n",
        "  # peak2peak\n",
        "  peak2peak = \n",
        "\n",
        "  # concatenar features mediante np.hstack\n",
        "  out = \n",
        "\n",
        "  # generar pd.DataFrame\n",
        "  db = pd.DataFrame(out, columns=['rms', 'peak', 'valley', 'peak2peak'])\n",
        "\n",
        "  return db\n",
        "\n",
        "# generar dataset de features a partir de X_raw\n",
        "db_ft = extract_features(X_raw)\n",
        "print( db_ft.head(n=5) )\n",
        "\n",
        "# obtener X_features mediante np.array.values\n",
        "X_features = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRekTUhn0N0c",
        "colab_type": "text"
      },
      "source": [
        "Finalmente, podemos visualizar la distribución que presentan estos features en cada uno de los estados, para hacernos una idea de que tan útiles podrían resultar para el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TK0aPt3yzZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scatter plot RMS\n",
        "plt.figure( figsize=(15, 4) )\n",
        "plt.xlabel('Sample'); plt.ylabel('RMS')\n",
        "\n",
        "plt.scatter(db_ft.index, db_ft['rms'],\n",
        "            c=list(labels), s=10, cmap='winter')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}